{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Notebook working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dai/evard/car-llm/scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dai/evard/car-llm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/evard/car-llm/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dai/evard/car-llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_scroll {\n",
       "    overflow-y: auto;\n",
       "    max-height: 100px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output_scroll {\n",
    "    overflow-y: auto;\n",
    "    max-height: 100px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine tune dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-05 19:22:39,720] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-02-05 19:22:42] llamafactory.hparams.parser:380 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2025-02-05 19:22:43,382 >> loading configuration file config.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-05 19:22:43,387 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,521 >> loading file vocab.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,521 >> loading file merges.txt from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,522 >> loading file tokenizer.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,522 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,522 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:43,522 >> loading file tokenizer_config.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-02-05 19:22:43,853 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2025-02-05 19:22:44,429 >> loading configuration file preprocessor_config.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2025-02-05 19:22:44,552 >> loading configuration file preprocessor_config.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2025-02-05 19:22:44,552 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file vocab.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file merges.txt from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file tokenizer.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-05 19:22:44,675 >> loading file tokenizer_config.json from cache at /home/dai/evard/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/a28a094eb66a9f2ac70eef346f040d8a79977472/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-02-05 19:22:45,003 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:755] 2025-02-05 19:22:45,801 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-7B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-02-05 19:22:45] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
      "[INFO|2025-02-05 19:22:45] llamafactory.data.loader:157 >> Loading dataset lingoQA_action.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 265323 examples [00:03, 68530.57 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 265323/265323 [00:02<00:00, 112300.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-02-05 19:22:52] llamafactory.data.loader:157 >> Loading dataset lingoQA_scenery.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 148506 examples [00:01, 76753.14 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 148506/148506 [00:01<00:00, 114608.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "llamafactory-cli train configs/finetuning_configs/qwen2vl_lora_sft_262144_res.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You must activate the http image server and the api before making requests to the api:\n",
    "\n",
    "```bash\n",
    "tmux attach -t hhtp_image_server\n",
    "python -m http.server 8001\n",
    "``` \n",
    "and...\n",
    "\n",
    "```bash\n",
    "tmux attach -t api\n",
    "API_HOST=\"0.0.0.0\" API_PORT=\"8080\" llamafactory-cli api inference_configs/fixed_inference_image_resolution_589824/qwen2vl_lora_sft_lingoQA_scenery_1000.yaml\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.inference_api import InferenceAPI\n",
    "\n",
    "inference_pipeline = InferenceAPI(\n",
    "    model_config_path=\"inference_configs/fixed_inference_image_resolution_589824/qwen2vl_lora_sft_lingoQA_scenery_1000.yaml\",\n",
    "    dataset_path=\"data/lingoQA_evaluation.json\",\n",
    "    json_output_path=\"data/predictions/chat_backend/fixed_inference_image_resolution_589824/qwen2vl_lora_sft_lingoQA_evaluation_500.json\",\n",
    ")\n",
    "\n",
    "\n",
    "inference_pipeline.run(max_dataset_len=2, num_threads = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.inference_chat import InferenceChat\n",
    "\n",
    "inference_pipeline = InferenceChat(\n",
    "    model_config_path=\"./inference_configs/fixed_inference_image_resolution_589824/qwen2vl_lora_sft_lingoQA_action_1000.yaml\",\n",
    "    json_output_path=\"./data/predictions/chat_backend/fixed_inference_image_resolution_589824/lingoQA_action_1000_predictions.json\",\n",
    "    dataset_path=\"./data/lingoQA_action.json\"\n",
    ")\n",
    "\n",
    "inference_pipeline.run(max_dataset_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: vLLM Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.inference_vllm import InferenceVLLM\n",
    "\n",
    "inference_pipeline = InferenceVLLM(\n",
    "    model_name_or_path=\"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    dataset=\"lingoQA_action\",\n",
    "    template=\"qwen2_vl\",\n",
    "    json_output_path=\"./data/predictions/chat_backend/fixed_inference_image_resolution_589824/lingoQA_action_1000_predictions.json\",\n",
    "    # adapter_name_or_path=\"./saves/qwen2_vl-7b/lora/sft/lingoQA_scenery_100\" # LoRA Adapter not yet supported, but merging beforehand works\n",
    ")\n",
    "\n",
    "inference_pipeline.run(max_dataset_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.save_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:19<00:00, 51.43 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 35566.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 62.4%\n"
     ]
    }
   ],
   "source": [
    "%%bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:19<00:00, 52.29 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 34699.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 61.8%\n"
     ]
    }
   ],
   "source": [
    "%%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 54.22 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37625.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 54.2%\n"
     ]
    }
   ],
   "source": [
    "%%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:28<00:00, 35.21 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 36249.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 53.900000000000006%\n"
     ]
    }
   ],
   "source": [
    "%%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 53.11 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 38820.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 53.2%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:20<00:00, 49.35 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37392.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 50.6%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 53.42 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 38294.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 58.8%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 54.99 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37041.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 63.2%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:19<00:00, 51.61 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 36754.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 58.5%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 52.87 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37429.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 55.50000000000001%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 53.78 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37410.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 55.800000000000004%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 54.29 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 37515.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 61.9%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 54.02 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 38041.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 61.6%\n",
      "Loaded 500 references.\n",
      "Loaded 1000 predictions.\n",
      "Matched 1000 predictions with references.\n",
      "WARNING! You are evaluating on a subset of the LingoQA benchmark. Please check your input file for missing or mis-matched examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:18<00:00, 54.31 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 36768.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall benchmark score is 45.9%\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
